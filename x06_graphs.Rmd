---
title: "05_graphs"
author: "Laura Botzet"
date: "10 August 2019"
output: 
  html_document:
    code_folding: "hide"
---

## Graphs {.tabset}

### Library
```{r Library}
#library(formr)
#library(lmerTest)
#library(effects)
#library(lme4)
#library(sjstats)
library(formr)
library(effects)
library(effectsize)
library(lme4)
library(sjstats)
library(lmerTest)
library(ggplot2)
library(tidyr)
library(ggpubr)
library(RColorBrewer)
library(dplyr)


apatheme = theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5))
```


### Data
Load selected data based on 03_codebook
```{r}
data_included_documented = read.csv(file = "data_included_documented.csv")[,-1]
data_included_documented = data_included_documented %>% filter(!is.na(age))
```

### Figure 2 {.tabset}
Effect size estimates of linear age effects on partner preferences for main analyses and three robustness analyses.

#### Main Analyses
```{r}
model_ippref_kind_supportive = standardize(
  lmer(ippref_kind_supportive ~ age + (1 | country),
       data = data_included_documented))

model_ippref_level_kind_supportive = standardize(
  lmer(ippref_level_kind_supportive ~ age + (1 | country),
       data = data_included_documented))

model_ippref_attractiveness = standardize(
  lmer(ippref_attractiveness ~ age + (1 | country),
       data = data_included_documented))

model_ippref_level_attractiveness = standardize(
  lmer(ippref_level_attractiveness ~ age + (1 | country),
       data = data_included_documented))

model_ippref_financiallysecure_sussessful = standardize(
  lmer(ippref_financiallysecure_sussessful ~ age + (1 | country),
       data = data_included_documented))

model_ippref_level_financiallysecure_sussessful = standardize(
  lmer(ippref_level_financiallysecure_sussessful ~ age + (1 | country),
       data = data_included_documented))

model_ippref_confident_assertive = standardize(
  lmer(ippref_confident_assertive ~ age + (1 | country),
       data = data_included_documented))

model_ippref_level_confident_assertive = standardize(
  lmer(ippref_level_confident_assertive ~ age + (1 | country),
       data = data_included_documented))

model_ippref_intelligence_educated = standardize(
  lmer(ippref_intelligence_educated ~ age + (1 | country),
       data = data_included_documented))

model_ippref_level_intelligence_educated = standardize(
  lmer(ippref_level_intelligence_educated ~ age + (1 | country),
       data = data_included_documented))
```

#### Robustness Analyses: Random Intercept for Language, Answer Accuracy missing
```{r}
model_ippref_kind_supportive1 = standardize(
  lmer(ippref_kind_supportive ~ age + (1 | language),
       data = data_included_documented))

model_ippref_level_kind_supportive1 = standardize(
  lmer(ippref_level_kind_supportive ~ age + (1 | language),
       data = data_included_documented))

model_ippref_attractiveness1 = standardize(
  lmer(ippref_attractiveness ~ age + (1 | language),
       data = data_included_documented))

model_ippref_level_attractiveness1 = standardize(
  lmer(ippref_level_attractiveness ~ age + (1 | language),
       data = data_included_documented))

model_ippref_financiallysecure_sussessful1 = standardize(
  lmer(ippref_financiallysecure_sussessful ~ age + (1 | language),
       data = data_included_documented))

model_ippref_level_financiallysecure_sussessful1 = standardize(
  lmer(ippref_level_financiallysecure_sussessful ~ age + (1 | language),
       data = data_included_documented))

model_ippref_confident_assertive1 = standardize(
  lmer(ippref_confident_assertive ~ age + (1 | language),
       data = data_included_documented))

model_ippref_level_confident_assertive1 = standardize(
  lmer(ippref_level_confident_assertive ~ age + (1 | language),
       data = data_included_documented))

model_ippref_intelligence_educated1 = standardize(
  lmer(ippref_intelligence_educated ~ age + (1 | language),
       data = data_included_documented))

model_ippref_level_intelligence_educated1 = standardize(
  lmer(ippref_level_intelligence_educated ~ age + (1 | language),
       data = data_included_documented))
```

#### Robustness Analyses, excluding is.na(answer_accuracy)
```{r}
model_ippref_kind_supportive2 = standardize(
  lmer(ippref_kind_supportive ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_kind_supportive2 = standardize(
  lmer(ippref_level_kind_supportive ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_attractiveness2 = standardize(
  lmer(ippref_attractiveness ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_attractiveness2 = standardize(
  lmer(ippref_level_attractiveness ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_financiallysecure_sussessful2 = standardize(
  lmer(ippref_financiallysecure_sussessful ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_financiallysecure_sussessful2 = standardize(
  lmer(ippref_level_financiallysecure_sussessful ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_confident_assertive2 = standardize(
  lmer(ippref_confident_assertive ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_confident_assertive2 = standardize(
  lmer(ippref_level_confident_assertive ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_intelligence_educated2 = standardize(
  lmer(ippref_intelligence_educated ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_intelligence_educated2 = standardize(
  lmer(ippref_level_intelligence_educated ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))
```

#### Robustness Analyses, random intercept for language & excluding is.na(answer_accuracy)
```{r}
model_ippref_kind_supportive3 = standardize(
  lmer(ippref_kind_supportive ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_kind_supportive3 = standardize(
  lmer(ippref_level_kind_supportive ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_attractiveness3 = standardize(
  lmer(ippref_attractiveness ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_attractiveness3 = standardize(
  lmer(ippref_level_attractiveness ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_financiallysecure_sussessful3 = standardize(
  lmer(ippref_financiallysecure_sussessful ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_financiallysecure_sussessful3 = standardize(
  lmer(ippref_level_financiallysecure_sussessful ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_confident_assertive3 = standardize(
  lmer(ippref_confident_assertive ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_confident_assertive3 = standardize(
  lmer(ippref_level_confident_assertive ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_intelligence_educated3 = standardize(
  lmer(ippref_intelligence_educated ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_ippref_level_intelligence_educated3 = standardize(
  lmer(ippref_level_intelligence_educated ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))
```


#### Combine {.active}
```{r}
library(coefplot)
x = multiplot(model_ippref_kind_supportive, model_ippref_level_kind_supportive,
              model_ippref_attractiveness, model_ippref_level_attractiveness,
              model_ippref_financiallysecure_sussessful,
              model_ippref_level_financiallysecure_sussessful,
              model_ippref_confident_assertive, model_ippref_level_confident_assertive,
              model_ippref_intelligence_educated, model_ippref_level_intelligence_educated,
              model_ippref_kind_supportive1, model_ippref_level_kind_supportive1,
              model_ippref_attractiveness1, model_ippref_level_attractiveness1,
              model_ippref_financiallysecure_sussessful1,
              model_ippref_level_financiallysecure_sussessful1,
              model_ippref_confident_assertive1, model_ippref_level_confident_assertive1,
              model_ippref_intelligence_educated1, model_ippref_level_intelligence_educated1,
              model_ippref_kind_supportive2, model_ippref_level_kind_supportive2,
              model_ippref_attractiveness2, model_ippref_level_attractiveness2,
              model_ippref_financiallysecure_sussessful2,
              model_ippref_level_financiallysecure_sussessful2,
              model_ippref_confident_assertive2, model_ippref_level_confident_assertive2,
              model_ippref_intelligence_educated2, model_ippref_level_intelligence_educated2,
              model_ippref_kind_supportive3, model_ippref_level_kind_supportive3,
              model_ippref_attractiveness3, model_ippref_level_attractiveness3,
              model_ippref_financiallysecure_sussessful3,
              model_ippref_level_financiallysecure_sussessful3,
              model_ippref_confident_assertive3, model_ippref_level_confident_assertive3,
              model_ippref_intelligence_educated3, model_ippref_level_intelligence_educated3,
              intercept = FALSE, coefficients = c("age"), plot = F)
        
x = data.frame(x) %>%
  mutate(Model = ifelse((Model %contains% "model_ippref_kind_supportive"),
                        "H1a) Importance\nKindness-Supportiveness",
                        ifelse((Model %contains% "model_ippref_level_kind_supportive"),
                               "H1b) Level\nKindness-Supportiveness",
                               ifelse((Model %contains% "model_ippref_attractiveness"),
                                       "H2a) Importance\nAttractiveness",
                 ifelse((Model %contains% "model_ippref_level_attractiveness"),
                        "H2b) Level\nAttractiveness",
                 ifelse((Model %contains% "model_ippref_financiallysecure_sussessful"),
                        "H3a) Importance\nFinancial Security-Successfulness",
                 ifelse((Model %contains% "model_ippref_level_financiallysecure_sussessful"),
                        "H3b) Level\nFinancial Security-Successfulness",
                               ifelse((Model %contains% "model_ippref_confident_assertive"),
                                      "H4a) Importance\nConfidence-Assertiveness",
                 ifelse((Model %contains%  "model_ippref_level_confident_assertive"),
                        "H4b) Level\nConfidence-Assertiveness",
                        ifelse((Model %contains% "model_ippref_intelligence_educated"),
                               "H5a) Importance\nEducation-Intelligence",
                 ifelse((Model %contains% "model_ippref_level_intelligence_educated"),
                        "H5b) Level\nEducation-Intelligence", NA)))))))))),
         Model = as.factor(Model))

x$Model = factor(x$Model,
                 levels = rev(c("H1a) Importance\nKindness-Supportiveness",
                            "H1b) Level\nKindness-Supportiveness",
                            "H2a) Importance\nAttractiveness",
                            "H2b) Level\nAttractiveness",
                            "H3a) Importance\nFinancial Security-Successfulness",
                            "H3b) Level\nFinancial Security-Successfulness",
                            "H4a) Importance\nConfidence-Assertiveness",
                            "H4b) Level\nConfidence-Assertiveness",
                            "H5a) Importance\nEducation-Intelligence",
                            "H5b) Level\nEducation-Intelligence")))

x$Group = c("Kindness-Supportiveness", "Kindness-Supportiveness",
            "Attractiveness", "Attractiveness",
            "Financial Security-Successfulness", "Financial Security-Successfulness",
            "Confidence-Assertiveness", "Confidence-Assertiveness",
            "Education-Intelligence", "Education-Intelligence",
            "Kindness-Supportiveness", "Kindness-Supportiveness",
            "Attractiveness", "Attractiveness",
            "Financial Security-Successfulness", "Financial Security-Successfulness",
            "Confidence-Assertiveness", "Confidence-Assertiveness",
            "Education-Intelligence", "Education-Intelligence",
            "Kindness-Supportiveness", "Kindness-Supportiveness",
            "Attractiveness", "Attractiveness",
            "Financial Security-Successfulness", "Financial Security-Successfulness",
            "Confidence-Assertiveness", "Confidence-Assertiveness",
            "Education-Intelligence", "Education-Intelligence",
            "Kindness-Supportiveness", "Kindness-Supportiveness",
            "Attractiveness", "Attractiveness",
            "Financial Security-Successfulness", "Financial Security-Successfulness",
            "Confidence-Assertiveness", "Confidence-Assertiveness",
            "Education-Intelligence", "Education-Intelligence")

x$Analyses = c(
  rep("Main analyses", 10),
  rep("Robustness analyses 1",
      10),
  rep("Robustness analyses 2",
      10),
  rep("Robustness analyses 3",
      10))

multiplot = ggplot(x, aes(Model, Value)) +
  geom_pointrange(data=x, mapping=aes(x=Model, y=Value, ymin=HighOuter,
                                                ymax=LowOuter,
                                      color = Analyses),
                  position = position_dodge(width=0.3), size = 1) +
  scale_colour_manual(values = c("#86BDDB", "#3A8AC2", "#1F6EB3", "#084594")) +
  geom_hline(yintercept = 0, size = 1.5)  +
  geom_hline(yintercept = -.1, linetype = "dotted", size = 1.5) + 
  geom_hline(yintercept = .1, linetype = "dotted", size = 1.5) + 
  coord_flip() +
  apatheme +
  theme(text = element_text(size=15), axis.text.x = element_text(size = 10),
          axis.text.y = element_text(size = 10), legend.text = element_text(size = 10),
          plot.title = element_text(hjust = 0),
        legend.background = element_rect(fill="grey", 
                                  size=0.5, linetype="solid")) +
  labs(y = "Linear Age Effect", x = "Outcome")
multiplot  


jpeg("Figure2.jpeg", width = 1050, height = 700, pointsize = 100, quality = 100)
multiplot
dev.off()
```

### Figure 3 {.tabset}
Effect size estimates of linear and quadratic age effects on preference for parenting intention for main analyses and three robustness analyses


#### Main Analyses
```{r}
model_pref_imp_parenting = lmer(pref_imp_parenting ~ age + (1| country),
                                data = data_included_documented)

model_pref_imp_parenting_quadratic = lmer(pref_imp_parenting ~ age + I(age^2) +
                                  (1 | country),
                                data = data_included_documented %>% filter(!is.na(age)))

model_pref_level_parenting = lmer(pref_level_parenting ~ age + (1 | country),
                                data = data_included_documented)

model_pref_level_parenting_quadratic = lmer(pref_level_parenting ~ age + I(age^2) + (1| country),
                                data = data_included_documented %>% filter(!is.na(age)))

model_pref_imp_parenting = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting")

model_pref_level_parenting =
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting")

model_pref_imp_parenting_quadratic = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting_quadratic, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting_quadratic,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting_quadratic")

model_pref_level_parenting_quadratic = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting_quadratic, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting_quadratic,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting_quadratic")

```

#### Robustness Analyses 1: Random Intercept for Language, Answer Accuracy missing
```{r}
model_pref_imp_parenting1 = lmer(pref_imp_parenting ~ age + (1| language),
                                data = data_included_documented)

model_pref_imp_parenting_quadratic1 = lmer(pref_imp_parenting ~ age + I(age^2) +
                                  (1 | language),
                                data = data_included_documented %>% filter(!is.na(age)))

model_pref_level_parenting1 = lmer(pref_level_parenting ~ age + (1 | language),
                                data = data_included_documented)

model_pref_level_parenting_quadratic1 = lmer(pref_level_parenting ~ age + I(age^2) +
                                               (1| language),
                                data = data_included_documented %>% filter(!is.na(age)))

model_pref_imp_parenting1 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting1, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting1,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting1")

model_pref_level_parenting1 =
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting1, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting1,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting1")

model_pref_imp_parenting_quadratic1 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting_quadratic1, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting_quadratic1,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting_quadratic1")

model_pref_level_parenting_quadratic1 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting_quadratic1, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting_quadratic1,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting_quadratic1")

```

#### Robustness Analyses, excluding is.na(answer_accuracy)
```{r}
model_pref_imp_parenting2 = lmer(pref_imp_parenting ~ age + (1| country),
                                data = data_included_documented %>%
         filter(!is.na(answer_accuracy)))

model_pref_imp_parenting_quadratic2 = lmer(pref_imp_parenting ~ age + I(age^2) +
                                  (1 | country),
                                data = data_included_documented %>% filter(!is.na(age)) %>%
         filter(!is.na(answer_accuracy)))

model_pref_level_parenting2 = lmer(pref_level_parenting ~ age + (1 | country),
                                data = data_included_documented %>%
         filter(!is.na(answer_accuracy)))

model_pref_level_parenting_quadratic2 = lmer(pref_level_parenting ~ age + I(age^2) + (1| country),
                                data = data_included_documented %>% filter(!is.na(age)) %>%
         filter(!is.na(answer_accuracy)))

model_pref_imp_parenting2 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting2, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting2,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting2")

model_pref_level_parenting2 =
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting2, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting2,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting2")

model_pref_imp_parenting_quadratic2 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting_quadratic2, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting_quadratic2,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting_quadratic2")

model_pref_level_parenting_quadratic2 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting_quadratic2, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting_quadratic2,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting_quadratic2")

```

#### Robustness Analyses, random intercept for language & excluding is.na(answer_accuracy)
```{r}
model_pref_imp_parenting3 = lmer(pref_imp_parenting ~ age + (1| language),
                                data = data_included_documented %>%
         filter(!is.na(answer_accuracy)))

model_pref_imp_parenting_quadratic3 = lmer(pref_imp_parenting ~ age + I(age^2) +
                                  (1 | language),
                                data = data_included_documented %>% filter(!is.na(age)) %>%
         filter(!is.na(answer_accuracy)))

model_pref_level_parenting3 = lmer(pref_level_parenting ~ age + (1 | language),
                                data = data_included_documented %>%
         filter(!is.na(answer_accuracy)))

model_pref_level_parenting_quadratic3 = lmer(pref_level_parenting ~ age + I(age^2) +
                                               (1| language),
                                data = data_included_documented %>% filter(!is.na(age)) %>%
         filter(!is.na(answer_accuracy)))

model_pref_imp_parenting3 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting3, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting3,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting3")

model_pref_level_parenting3 =
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting3, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting3,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting3")

model_pref_imp_parenting_quadratic3 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_imp_parenting_quadratic3, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_imp_parenting_quadratic3,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_imp_parenting_quadratic3")

model_pref_level_parenting_quadratic3 = 
  left_join(
    as.data.frame(
      standardize_parameters(model_pref_level_parenting_quadratic3, method = "basic")),
    as.data.frame(ci(standardize_parameters(model_pref_level_parenting_quadratic3,
                                            method = "basic", ci = 0.995))),
    by = "Parameter") %>%
  mutate(Model = "model_pref_level_parenting_quadratic3")

```


#### Combine {.active}
```{r}
x = rbind(
  model_pref_imp_parenting, model_pref_level_parenting,
  model_pref_imp_parenting_quadratic, model_pref_level_parenting_quadratic,
  model_pref_imp_parenting1, model_pref_level_parenting1,
  model_pref_imp_parenting_quadratic1, model_pref_level_parenting_quadratic1,
  model_pref_imp_parenting2, model_pref_level_parenting2,
  model_pref_imp_parenting_quadratic2, model_pref_level_parenting_quadratic2,
  model_pref_imp_parenting3, model_pref_level_parenting3,
  model_pref_imp_parenting_quadratic3, model_pref_level_parenting_quadratic3
  ) %>%
  filter(Parameter != "(Intercept)") %>%
  rename(Value = Std_Coefficient,
         Coefficient = Parameter,
         HighOuter = CI_high,
         LowOuter = CI_low)
        
x = data.frame(x) %>%
  mutate(Model = ifelse((Model %contains% "model_pref_imp_parenting_quadratic"),
                         "H7a) Quadratic Model\nImportance Shared Preference for Number of Children",
                 ifelse((Model %contains% "model_pref_level_parenting_quadratic"),
                         "H7b) Quadratic Model\nLevel Partner's Intention to Become a Parent",
                 ifelse((Model %contains% "model_pref_imp_parenting"),
                        "H6a) Linear Model\nImportance Shared Preference for Number of Children",
                 ifelse((Model %contains% "model_pref_level_parenting"),
                         "H6b) Linear Model\nLevel Partner's Intention to Become a Parent",
                        NA)))),
         Model = as.factor(Model),
         Coefficient = ifelse(Coefficient == "age", "Linear effects", "Quadratic effects"))
                        
                        
                        
                        
                        
x$Model = factor(x$Model,
                 levels = rev(c("H6a) Linear Model\nImportance Shared Preference for Number of Children",
                            "H6b) Linear Model\nLevel Partner's Intention to Become a Parent",
                            "H7a) Quadratic Model\nImportance Shared Preference for Number of Children",
                            "H7b) Quadratic Model\nLevel Partner's Intention to Become a Parent")))


x$Analyses = c(
  rep("Main analyses", 6),
  rep("Robustness analyses 1", 6),
  rep("Robustness analyses 2", 6),
  rep("Robustness analyses 3", 6))

multiplot = ggplot(x, aes(Model, Value)) +
  geom_pointrange(data=x, mapping=aes(x=Model, y=Value, ymin=HighOuter,
                                      ymax=LowOuter, color = Analyses,
                                      shape = Coefficient),
                  position = position_dodge(width=0.3), size = 1) +
  scale_colour_manual(values = c("#86BDDB", "#3A8AC2", "#1F6EB3", "#084594")) +
  geom_hline(yintercept = 0, size = 1.5)  +
  coord_flip() +
  apatheme +
  theme(text = element_text(size=15), axis.text.x = element_text(size = 10),
          axis.text.y = element_text(size = 10), legend.text = element_text(size = 10),
          plot.title = element_text(hjust = 0),
        legend.background = element_rect(fill="grey", 
                                  size=0.5, linetype="solid")) +
  labs(y = "Age Effect", x = "Outcome")
multiplot  


jpeg("Figure3.jpeg", width = 1500, height = 700, pointsize = 100, quality = 100)
multiplot
dev.off()

```

### Figure 4 {.tabset}
Preference for parenting intention by a linear and quadratic effect of age controlled for a random intercept for country.
#### Models
```{r}
effects1 = lmer(pref_imp_parenting ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects1 = as.data.frame(effects1$age)

n = data_included_documented %>%
  filter(!is.na(pref_imp_parenting)) %>%
  count()

plot1 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = pref_imp_parenting), alpha = 0.05) +
  geom_line(data = effects1, aes(x = age, y = fit)) +
  geom_ribbon(data = effects1, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nShared Preference for Children",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H6a) Linear Effect of Age on Importance of\nShared Preference for Children (n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot1


effects2 = lmer(pref_imp_parenting ~ age + I(age^2) + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects2 = as.data.frame(effects2$`I(age^2)`)

n = data_included_documented %>%
  filter(!is.na(pref_imp_parenting)) %>%
  count()

plot2 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = pref_imp_parenting), alpha = 0.05) +
  geom_line(data = effects2, aes(x = age, y = fit)) +
  geom_ribbon(data = effects2, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nShared Preference for Children",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H7a) Linear and Quadratic Effect of Age on Importance of\nShared Preference for Children (n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot2

effects3 = lmer(pref_level_parenting ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects3 = as.data.frame(effects3$age)

n = data_included_documented %>%
  filter(!is.na(pref_level_parenting)) %>%
  count()

plot3 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = pref_level_parenting), alpha = 0.05) +
  geom_line(data = effects3, aes(x = age, y = fit)) +
  geom_ribbon(data = effects3, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nPartner's Intention to Become a Parent",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H6b) Linear Effect of Age on Level of\nPartner's Intention to Become a Parent (n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot3


effects4 = lmer(pref_level_parenting ~ age + I(age^2) + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects4 = as.data.frame(effects4$`I(age^2)`)

n = data_included_documented %>%
  filter(!is.na(pref_level_parenting)) %>%
  count()

plot4 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = pref_level_parenting), alpha = 0.05) +
  geom_line(data = effects4, aes(x = age, y = fit)) +
  geom_ribbon(data = effects4, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nPartner's Intention to Become a Parent",
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H7b) Linear and Quadratic Effect of Age on Level of\nPartner's Intention to Become a Parent (n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  coord_cartesian(ylim=c(-0.5, 6.5)) +
  apatheme
plot4

```

#### Arrange Plots {.active}
```{r}
plot5 = ggarrange(
  plot1, plot3,
  plot2, plot4,
  ncol = 2, nrow = 2)
plot5

jpeg("Figure4.jpeg", width = 800, height = 500, pointsize = 100, quality = 100)
plot5
dev.off()
```

### Figure 5 {.tabset}
Effect size estimates of linear effects on on age-range deemed acceptable and on youngest and oldest age deemed acceptable for main analyses and three robustness analyses.
#### Main Analyses
```{r}
model_pref_age_range = standardize(
  lmer(pref_age_range ~ age + (1 | country),
       data = data_included_documented))

model_pref_age_min_rel = standardize(
  lmer(pref_age_min_rel ~ age + (1 | country),
       data = data_included_documented))

model_pref_age_max_rel = standardize(
  lmer(pref_age_max_rel ~ age + (1 | country),
       data = data_included_documented))
```

#### Robustness Analyses 1: Random Intercept for Language, Answer Accuracy missing
```{r}
model_pref_age_range1 = standardize(
  lmer(pref_age_range ~ age + (1 | language),
       data = data_included_documented))

model_pref_age_min_rel1 = standardize(
  lmer(pref_age_min_rel ~ age + (1 | language),
       data = data_included_documented))

model_pref_age_max_rel1 = standardize(
  lmer(pref_age_max_rel ~ age + (1 | language),
       data = data_included_documented))
```

#### Robustness Analyses, excluding is.na(answer_accuracy)
```{r}
model_pref_age_range2 = standardize(
  lmer(pref_age_range ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_pref_age_min_rel2 = standardize(
  lmer(pref_age_min_rel ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_pref_age_max_rel2 = standardize(
  lmer(pref_age_max_rel ~ age + (1 | country),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))
```

#### Robustness Analyses, random intercept for language & excluding is.na(answer_accuracy)
```{r}
model_pref_age_range3 = standardize(
  lmer(pref_age_range ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_pref_age_min_rel3 = standardize(
  lmer(pref_age_min_rel ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))

model_pref_age_max_rel3 = standardize(
  lmer(pref_age_max_rel ~ age + (1 | language),
       data = data_included_documented %>%
         filter(!is.na(answer_accuracy))))
```

#### Combine {.active}
```{r}
library(coefplot)
x = multiplot(model_pref_age_range, model_pref_age_min_rel, model_pref_age_max_rel,
              model_pref_age_range1, model_pref_age_min_rel1, model_pref_age_max_rel1,
              model_pref_age_range2, model_pref_age_min_rel2, model_pref_age_max_rel2,
              model_pref_age_range3, model_pref_age_min_rel3, model_pref_age_max_rel3,
              intercept = FALSE, coefficients = c("age"), plot = F)
        
x = data.frame(x) %>%
  mutate(Model = ifelse((Model %contains% "model_pref_age_range"),
                        "H8) Age-range deemed acceptable",
                        ifelse((Model %contains% "model_pref_age_min_rel"),
                               "H9) Youngest Age deemed acceptable",
                               "H10) Oldest Age deemed acceptable")),
         Model = as.factor(Model))

x$Model = factor(x$Model,
                 levels = rev(c("H8) Age-range deemed acceptable",
                            "H9) Youngest Age deemed acceptable",
                            "H10) Oldest Age deemed acceptable")))


x$Analyses = c(
  rep("Main analyses", 3),
  rep("Robusness analyses 1", 3),
  rep("Robusness analyses 2", 3),
  rep("Robusness analyses 3", 3))

multiplot = ggplot(x, aes(Model, Value)) +
  geom_pointrange(data=x, mapping=aes(x=Model, y=Value, ymin=HighOuter,
                                                ymax=LowOuter,
                                      color = Analyses),
                  position = position_dodge(width=0.3), size = 1) +
  scale_colour_manual(values = c("#86BDDB", "#3A8AC2", "#1F6EB3", "#084594")) +
  geom_hline(yintercept = 0, size = 1.5)  +
  geom_hline(yintercept = -.1, linetype = "dotted", size = 1.5) + 
  geom_hline(yintercept = .1, linetype = "dotted", size = 1.5) + 
  coord_flip() +
  apatheme +
  theme(text = element_text(size=15), axis.text.x = element_text(size = 10),
          axis.text.y = element_text(size = 10), legend.text = element_text(size = 10),
          plot.title = element_text(hjust = 0),
        legend.background = element_rect(fill="grey", 
                                  size=0.5, linetype="solid")) +
  labs(y = "Linear Age Effect", x = "Outcome")
multiplot  


jpeg("Figure5.jpeg", width = 1050, height = 700, pointsize = 100, quality = 100)
multiplot
dev.off()
```

### Figure 6 {.tabset}

#### Models
Youngest and oldest age deemed acceptable by own age (n = 12,154).

### Tolerated Age Range
```{r}
n = data_included_documented %>%
  filter(!is.na(pref_age_range)) %>%
  count()

plot1 = ggplot(data_included_documented %>%
         select(age, pref_age_range),
       aes(age, pref_age_range)) +
  geom_jitter(alpha = 0.05) +
  stat_smooth(method = "lm", level = 0.995) +
  scale_x_continuous(name = "\nAge",
                     limits = c(17,70),
                     labels = c(20,30,40,50,60),
                     breaks = c(20,30,40,50,60)) +
  scale_y_continuous(name = "Age-Range Deemed Acceptable\n",
                     labels = c(0,10,20,30,40,50,60,70,80,90,100),
                     breaks = c(0,10,20,30,40,50,60,70,80,90,100)) +
  apatheme + 
  ggtitle(paste0("H8) Age-Range Deemed Acceptable\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")"))
```

#### Women's tolerated age for older and younger partner
```{r}
plot2 = ggplot(data_included_documented %>%
         select(age, pref_age_min_rel, pref_age_max_rel) %>%
         mutate(pref_age_min_rel = -pref_age_min_rel),
       aes(age, pref_age_min_rel)) +
  stat_smooth(aes(x = age, y = pref_age_min_rel),
              method = "lm", level = 0.995,
              color = brewer.pal(9,"Blues")[8],
              fill = brewer.pal(9,"Blues")[8], alpha = .5) +
  stat_smooth(aes(x = age, y = pref_age_max_rel),
              method = "lm", level = 0.995,
              color = brewer.pal(9,"Reds")[8],
              fill = brewer.pal(9,"Reds")[8], alpha = .5) +
  scale_x_continuous(name = "\nAge") +
  scale_y_continuous(name = "Age-Range Deemed Acceptable\n") +
  apatheme + 
  ggtitle(paste0("H9) & H10) Youngest and Oldest Age Deemed Accptable\nRelative to Own Age \n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")"))
plot2


```

#### Combine Plots {.active}
```{r}
plot3 = ggarrange(plot1 , plot2 ,
          ncol = 2, nrow = 1)

plot3

jpeg("Figure6.jpeg", width = 800, height = 300, pointsize = 100, quality = 100)
plot3
dev.off()
```

### Figure S2
#### Models
Hypothesis 1a): There is no link between age and women’s importance ratings for partner’s kindness and supportiveness.

```{r}
effects1 = lmer(ippref_kind_supportive ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects1 = as.data.frame(effects1$age)

n = data_included_documented %>%
  filter(!is.na(ippref_kind_supportive)) %>%
  count()

plot1 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_kind_supportive), alpha = 0.05) +
  geom_line(data = effects1, aes(x = age, y = fit)) +
  geom_ribbon(data = effects1, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nKindness-Supportiveness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H1a) Importance of Kindness-Supportiveness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot1

```

Hypothesis 1b): There is no link between age and women’s preferences for the levels of kindness and supportiveness.
```{r}
effects2 = lmer(ippref_level_kind_supportive ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects2 = as.data.frame(effects2$age)

n = data_included_documented %>%
  filter(!is.na(ippref_level_kind_supportive)) %>%
  count()

plot2 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_kind_supportive), alpha = 0.05) +
  geom_line(data = effects2, aes(x = age, y = fit)) +
  geom_ribbon(data = effects2, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nKindness-Supportiveness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H1b) Level of Kindness-Supportiveness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot2
```

Hypothesis 2a): There is no link between age and women’s importance ratings for partner’s attractiveness.

```{r}
effects3 = lmer(ippref_attractiveness ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects3 = as.data.frame(effects3$age)

n = data_included_documented %>%
  filter(!is.na(ippref_attractiveness)) %>%
  count()

plot3 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_attractiveness), alpha = 0.05) +
  geom_line(data = effects3, aes(x = age, y = fit)) +
  geom_ribbon(data = effects3, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nAttractiveness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H2a) Importance of Attractiveness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot3
```

Hypothesis 2b): There is no link between age and women’s preferences for the levels of attractiveness.
```{r}
effects4 = lmer(ippref_level_attractiveness ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects4 = as.data.frame(effects4$age)

n = data_included_documented %>%
  filter(!is.na(ippref_level_attractiveness)) %>%
  count()

plot4 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_level_attractiveness), alpha = 0.05) +
  geom_line(data = effects4, aes(x = age, y = fit)) +
  geom_ribbon(data = effects4, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nAttractiveness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H2b) Level of Attractiveness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot4
```

Hypothesis 3a): There is no link between age and women’s importance ratings for partner’s financial security and successfulness.

```{r}
effects5 = lmer(ippref_financiallysecure_sussessful ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects5 = as.data.frame(effects5$age)

n = data_included_documented %>%
  filter(!is.na(ippref_financiallysecure_sussessful)) %>%
  count()

plot5 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_financiallysecure_sussessful), alpha = 0.05) +
  geom_line(data = effects5, aes(x = age, y = fit)) +
  geom_ribbon(data = effects5, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nFinancial Security-Successfulness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H3a) Importance of Financial Security-Successfulness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot5
```

Hypothesis 3b): There is no link between age and women’s preferences for the levels of financial security and successfulness.
```{r}
effects6 = lmer(ippref_level_financiallysecure_sussessful ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects6 = as.data.frame(effects6$age)

n = data_included_documented %>%
  filter(!is.na(ippref_level_financiallysecure_sussessful)) %>%
  count()

plot6 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_level_financiallysecure_sussessful), alpha = 0.05) +
  geom_line(data = effects6, aes(x = age, y = fit)) +
  geom_ribbon(data = effects6, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nFinancial Security-Successfulness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H3b) Level of Financial Security-Successfulness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot6
```

Hypothesis 4a): There is no link between age and women’s importance ratings for partner’s confidence and assertiveness.

```{r}
effects7 = lmer(ippref_confident_assertive ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects7 = as.data.frame(effects7$age)

n = data_included_documented %>%
  filter(!is.na(ippref_confident_assertive)) %>%
  count()

plot7 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_confident_assertive), alpha = 0.05) +
  geom_line(data = effects7, aes(x = age, y = fit)) +
  geom_ribbon(data = effects7, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nConfidence-Assertiveness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H4a) Importance of Confidence-Assertiveness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot7
```

Hypothesis 4b): There is no link between age and women’s preferences for the levels of confidence and assertiveness.
```{r}
effects8 = lmer(ippref_level_confident_assertive ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects8 = as.data.frame(effects8$age)

n = data_included_documented %>%
  filter(!is.na(ippref_level_confident_assertive)) %>%
  count()

plot8 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_level_confident_assertive), alpha = 0.05) +
  geom_line(data = effects8, aes(x = age, y = fit)) +
  geom_ribbon(data = effects8, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nConfidence-Assertiveness",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H4b) Level of Confidence-Assertiveness\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot8
```

Hypothesis 5a): There is no link between age and women’s importance ratings for partner’s education and intelligence.

```{r}
effects9 = lmer(ippref_intelligence_educated ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects9 = as.data.frame(effects9$age)

n = data_included_documented %>%
  filter(!is.na(ippref_intelligence_educated)) %>%
  count()

plot9 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_intelligence_educated), alpha = 0.05) +
  geom_line(data = effects9, aes(x = age, y = fit)) +
  geom_ribbon(data = effects9, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Importance\nEducation-Intelligence",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H5a) Importance of Education-Intelligence\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot9
```

Hypothesis 5b): There is no link between age and women’s preferences for the levels of education and intelligence.
```{r}
effects10 = lmer(ippref_level_intelligence_educated ~ age + (1 | country),
             data = data_included_documented) %>%
  allEffects(confidence.level = 0.995, xlevels = 49) %>%
  as.data.frame()

effects10 = as.data.frame(effects10$age)

n = data_included_documented %>%
  filter(!is.na(ippref_level_intelligence_educated)) %>%
  count()

plot10 = ggplot() +
  geom_jitter(data = data_included_documented, aes(x = age, y = ippref_level_intelligence_educated), alpha = 0.05) +
  geom_line(data = effects10, aes(x = age, y = fit)) +
  geom_ribbon(data = effects10, aes(x = age, ymin = lower, ymax = upper),
              fill = brewer.pal(9,"Blues")[8], alpha = 0.8) +
  scale_x_continuous(name = "Age") +
  scale_y_continuous(name = "Level\nEducation-Intelligence",
                     limits = c(-0.5,6.5),
                     breaks = c(0,1,2,3,4,5,6)) +
  ggtitle(paste0(
      "H5b) Level of Education-Intelligence\n(n = ",
      prettyNum(n$n, big.mark = ",", preserve.width = "none"),
      ")")) +
  apatheme
plot10
```

#### Arrange Plots {.active}
```{r}
plot11 = ggarrange(
  plot1, plot2,
  plot3, plot4,
  plot5, plot6,
  ncol = 2, nrow = 3)
plot11

jpeg("FigureS2_Part1.jpeg", width = 750, height = 750, pointsize = 100, quality = 100)
plot11
dev.off()

plot12 = ggarrange(
  plot7, plot8,
  plot9, plot10,
  ncol = 2, nrow = 2)
plot12

jpeg("FigureS2_Part2.jpeg", width = 750, height = 500, pointsize = 100, quality = 100)
plot12
dev.off()
```

### Figure S3 and Figure S4{.tabset}

#### Importance Parenting Intention {.tabset}
##### Code
```{r}
library(mgcv)         #This library has the additive model with smoothing function
library(stringr)      #To process strings in the function
library(sandwich)     #For robust standard errors
library(lmtest)       #To run linear tests
library(scales)

#OUTLINE
#Function 1 - rp(p)       Reformat p-value for printing on chart
#Function 2 - eval2()     Reads a variable name and assigns the values to another variable (useful for processing formulas and creating temporary variables)
#Function 3 - reg2()      Interrupted regression with entered formula plus cutoff point
#Function 4 - twolines()  Run two-lines test, relying on functions 1 and 2 as well

################################################################################
#Function 1 - rp(p)       Reformat p-value for printing on chart
rp=function(p) if (p<.0001)  return("p<.0001")  else  return(paste0("p=", sub("^(-?)0.", "\\1.", sprintf("%.4f", p))))

#Function 2 - eval2()   evaluate a string as a function
eval2=function(string) eval(parse(text=string))  #Function that evaluates a variable"
#if x.f="x1", then x=eval2(x.f)  will populate x with the values of x1

#Function 3 - two-line regression with glm() so that it has heteroskedastic robust standard errors
reg2=function(f,xc,graph=1,family="gaussian")
{
  #Syntax:
  #f: formula as in y~x1+x2+x3
  #The first predictor is the one the u-shape is tested on
  #xc: where to set the breakpoint, a number
  #link:
  #       Gaussian for OLS
  #       binomial for probit

  #(1) Extract variable names from formula
  #1.1 Get the formulas
  y.f=all.vars(f)[1]                  #DV
  x.f=all.vars(f)[2]                  #Variable on which the u-shape shall be tested
  #Number of variables
  var.count=length(all.vars(f))       #How many total variables, including y and x
  #Entire model, except the first predictor
  if (var.count>2) nox.f=drop.terms(terms(f),dropx=1,keep.response = T)


  #1.2 Grab the two key variables to be used, xu and yu
  xu=eval2(x.f)  #xu is the key predictor predicted to be u-shaped
  yu=eval2(y.f)  #yu is the dv

  #1.3 Replace formula for key predictor so that it accommodates possibly discrete values, gam() breaks down if x has few possible values unless one restricts it, done automatically
  #1.3.1 Count number of unique x values
  unique.x=length(unique(xu))   #How many unique values x has
  #1.3.2 New function segment for x
  sx.f=paste0("s(",x.f,",bs='cr', k=min(10,",unique.x,"))" )

  #1.4 xc is included in the first line
  xlow1  =ifelse(xu<=xc,xu-xc,0)   #xlow=x-xc when x<xc, 0 otherwise
  xhigh1=ifelse(xu>xc,xu-xc,0)     #xhigh=x when x<xmax, 0 otherwise
  high1 =ifelse(xu>xc,1,0)         #high dummy, allows interruption

  #1.5 Now include xc in second line
  xlow2=ifelse(xu<xc,xu-xc,0)
  xhigh2=ifelse(xu>=xc,xu-xc,0)
  high2=ifelse(xu>=xc,1,0)

  #(2) Run interrupted regressions
  #2.1 Generate formulas  replacing the single predictor x, with the 3 new variables
  #If there were covariates, grab them an copy-paste them after the 3 new variables
  if (var.count>2)
  {
    glm1.f=update(nox.f,~ xlow1+xhigh1+high1+.)  #update takes a formula and adds elements to it, by putting the . at the end, it will paste the existing variables after the new 3 variables
    glm2.f=update(nox.f,~ xlow2+xhigh2+high2+.)
  }
  #If there were no covariates, just run the 3 variable model
  if (var.count==2)
  {
    glm1.f=as.formula("yu~ xlow1+xhigh1+high1")
    glm2.f=as.formula("yu~ xlow2+xhigh2+high2")
  }
  #2.2 Run them
  glm1=glm(as.formula(format(glm1.f)),family=family)
  glm2=glm(as.formula(format(glm2.f)),family=family)

  #2.3 Compute robust standard errors
  rob1=coeftest(glm1, vcov=vcovHC(glm1,"HC3"))  #Until 2018 03 20 I was using HC3, but they sometimes generate errors, so i switched it to HC1
  rob2=coeftest(glm2, vcov=vcovHC(glm2,"HC3"))

  #Sometimes HC3 gives NA values (for very sparse or extreme data), check and if that's the case change method
  msg=""
  if (is.na(rob1[2,4]))
  {
    rob1=coeftest(glm1, vcov=vcovHC(glm1,"HC1"))
    msg=paste0(msg,"\nFor line 1 the heteroskedastic standard errors HC3 resulted in an error thus we used HC1 instead.")
  }
  if (is.na(rob2[2,4]))
  {
    rob2=coeftest(glm2, vcov=vcovHC(glm2,"HC1"))
    msg=paste0(msg,"\nFor line 2 the heteroskedastic standard errors HC3 resulted in an error thus we used HC1 instead.")
  }

  #2.4 Slopes
  b1=as.numeric(rob1[2,1])
  b2=as.numeric(rob2[3,1])

  #2.5 Test statistics, z-values
  z1=as.numeric(rob1[2,3])
  z2=as.numeric(rob2[3,3])

  #2.6 p-values
  p1=as.numeric(rob1[2,4])
  p2=as.numeric(rob2[3,4])


  #3) Is the u-shape significant?
  u.sig =ifelse(b1*b2<0 & p1<.05 & p2<.05,1,0)

  #4) Plot results
  if (graph==1) {

    #4.1 General colors and parameters
    pch.dot=1          #Dot for scatterplot (data)
    #col.l1='blue2'     #Color of straight line 1
    col.l1='dodgerblue3'
    col.l2='firebrick'      #Color of straight line 2
    col.fit='gray50'   #Color of fitted smooth line

    col.div="green3"   #Color of vertical line
    lty.l1=1           #Type of line 1
    lty.l2=1           #Type of line 2
    lty.fit=2          #Type of smoothed line

    #4.2) Estimate smoother
    if (var.count>2)
    {
      gam.f=paste0(format(nox.f),"+",sx.f)   #add the modified smoother version of x into the formula
      gams=gam(as.formula(gam.f),link=link)  #now actually run the smoother
    }

    if (var.count==2)
    {
      gams=gam(as.formula(paste0("yu~",sx.f)),link=link)  #now actually run the smoother
    }

    #Get dots of raw data
    #4.3) If no covariates, there are two variables, and y.dots  is the y values
    if (var.count==2) yobs=yu

    #4.4) If covariates present, yobs is the fitted value with u(x) at mean, need new.data() with variables at means
    if (var.count>2) {

      #4.4.1) Put observed data into matrix
      data.obs=as.data.frame(matrix(nrow=length(xu),ncol=var.count))
      colnames(data.obs)=all.vars(f)
      #4.4.2 Populate the dataset with the observed variables
      for (i in 1:(var.count)) data.obs[,i]=eval(as.name(all.vars(f)[i]))


      #4.4.3) Drop observations with missing values on any of the variables
      data.obs=na.omit(data.obs)

      #4.4.4) Create data where u(x) is at sample means to get residuals based on rest of models to act as yobs
      #Recall: columns 1 & 2 have y and u(x) in obs.data
      data.xufixed    =data.obs
      data.xufixed[,2]=mean(data.obs[,2])   #Note, the 1st predictor, 2nd columns, is always the one hypothesized to be u-shaped
      #replace it with the mean value of the predictor
      #5.4.5) Create data where u(x) is obs, and all else at sample means
      data.otherfixed = data.obs     #start with original value
      #Replace all RHS with mean, except the u(x)
      #for (i in 3:var.count) data.otherfixed[,i]=mean(data.obs[,i])  #changed on 2018 11 23 to allow having factors() as predictors, their "midpoint" value is used, sometimes that value is more meaningfully a median point than others
      for (i in 3:var.count) {  #loop over covariates
        xt=sort(data.obs[,i]) #create auxiliary variable that has those values sorted
        n=length(xt)          #see how many observations there are
        xm=xt[round(n/2,0)]   #take the midpoint value (this will work with ordinal and factor data, but with factor it can be arbitrary)
        data.otherfixed[,i]=xm #Replace with that value in the dataset used for fitted value
      }
      #4.4.6) Get yobs with covariates
      #First the fitted value
      yhat.xufixed=predict.gam(gams,newdata = data.xufixed)
      #Substract fitted value from observed y, and shift it with constant so that it has same mean as original y
      yobs =yu-yhat.xufixed
      yobs=yobs+mean(yu)-mean(yobs)  #Adjust to have the same mean
    } #End if for covariates that requires computes y.obs instead of using real y.

    #4.5) First line (x,y) coordinates
    #     offset1=mean(yobs[xu<=xc])-min(xu)*b1-(xc-min(xu))/2*b1
    #     x.l1=c(min(xu),xc)
    #     y.l1=c(min(xu)*b1+offset1,xc*b1+offset1)
    #
    #     #4.6) First line (x,y) coordinates
    # 		offset2=mean(yobs[xu>=xc])-xc*b2-(max(xu)-xc)/2*b2
    # 		x.l2=c(xc,max(xu))
    # 		y.l2=c(xc*b2+offset2,max(xu)*b2+offset2)

    #4.7) Get yhat.smooth
    #Without covariates, just fit the observed data
    if (var.count==2)  yhat.smooth=predict.gam(gams)
    #With covariates, fit at observed means
    if (var.count>2)  yhat.smooth=predict.gam(gams,newdata = data.otherfixed)
    #Substract fitted value from observed y
    offset3 = mean(yobs-yhat.smooth)
    yhat.smooth=yhat.smooth+offset3

    #4.8) Coordinates for top and bottom end of chart
    y1   =max(yobs,yhat.smooth)  #highest point
    y0   =min(yobs,yhat.smooth)  #lowest point
    yr   =y1-y0                  #range
    y0   =y0-.3*yr               #new lowest. 30% lower

    #xs
    x1   =max(xu)
    x0   =min(xu)
    xr   =x1-x0

    #4.9) Plot
    #4.9.1) Figure out coordinates for arrows so that they fit

    par(mar=c(5.4,5.4,.5,2.1))
    plot(xu[xu<xc],yobs[xu<xc], cex =.75, pch = 16, col=alpha(col.l1, 0.1),las=1,
         ylim=c(y0,y1),
         xlim=c(min(xu),max(xu)),
         xlab="",
         ylab="",
         yaxt='n', ann=FALSE)  #Range of y has extra 30% to add labels
    points(xu[xu>xc],yobs[xu>xc], cex = .75, pch = 16, col=alpha(col.l2, 0.10))

    #Axis labels
    mtext(side=1,line=2.75,"Age",font=2)
    mtext(side=2,line=2.75,"\nImportance Rating\nShared Preference for Number of Children",font=2)
    axis(1, at = c(20, 25, 30, 35, 40, 45, 50, 55, 60, 65))
    axis(2, at = c(0, 1, 2, 3, 4, 5, 6))
    

    #4.10) Smoothed line
    #lines(xu[order(xu)],yhat.smooth[order(xu)],col=col.fit,lwd=2,lty=lty.fit)

    #4.10 - New 2018 05 25
    lines(xu[order(xu)],yhat.smooth[order(xu)],col=col.fit,lty=2,lwd=2)

    #4.10.2 Arrow 1
    xm1=(xc+x0)/2
    x0.arrow.1=xm1-.1*xr
    x1.arrow.1=xm1+.1*xr
    y0.arrow.1=y0+.1*yr
    y1.arrow.1=y0+.1*yr+ b1*(x1.arrow.1-x0.arrow.1)

    #Move arrow if it is too short
    if (x0.arrow.1<x0+.1*xr) x0.arrow.1=x0

    #Move arrow if it covers text
    gap.1=(min(y0.arrow.1,y1.arrow.1)-(y0+.1*yr))
    if (gap.1<0) {
      y0.arrow.1=y0.arrow.1-gap.1
      y1.arrow.1=y1.arrow.1-gap.1
    }

    arrows(x0=x0.arrow.1,x1=x1.arrow.1,y0=y0.arrow.1,y1=y1.arrow.1,col=col.l1,lwd=2)

    #4.10.3 Text under arrow 1
    xm1=max(xm1,x0+.20*xr)
    text(xm1,y0+.025*yr,
         paste0("Average Slope 1:\nb = ",round(b1,2),", ",rp(p1)),col=col.l1)

    #4.10.3 Arrow 2
    x0.arrow.2=xc+(x1-xc)/2-.1*xr
    x1.arrow.2=xc+(x1-xc)/2+.1*xr
    y0.arrow.2=y1.arrow.1
    y1.arrow.2=y0.arrow.2 + b2*(x1.arrow.2-x0.arrow.2)

    gap.2=(min(y0.arrow.2,y1.arrow.2)-(y0+.1*yr))
    if (gap.2<0) {
      y0.arrow.2=y0.arrow.2-gap.2
      y1.arrow.2=y1.arrow.2-gap.2
    }


    #Shorten arrow if it is too close to the end
    x1.arrow.2=min(x1.arrow.2,x1)
    if (x0.arrow.2<xc) x0.arrow.2=xc

    xm2=xc+(x1-xc)/2
    xm2=min(xm2,x1-.2*xr)


    arrows(x0=x0.arrow.2,x1=x1.arrow.2,y0=y0.arrow.2,y1=y1.arrow.2,col=col.l2,lwd=2)
    text(xm2,y0+.025*yr,
         paste0("Average Slope 2:\nb = ",round(b2,2),", ",rp(p2)),col=col.l2)



    #4.13 Division line
    lines(c(xc,xc),c(y0+.35*yr,y1),col=col.div,lty=lty.fit)
    text(xc,y0+.3*yr,round(xc,2),col=col.div)
    
  }#End: if  graph==1

  #5 list with results
  res=list(b1=b1,p1=p1,b2=b2,p2=p2,u.sig=u.sig,xc=xc,z1=z1,z2=z2,
           glm1=glm1,glm2=glm2,rob1=rob1,rob2=rob2,msg=msg)  #Output list with all those parameters, betas, z-values, p-values and significance for u

  if (graph==1) res$yhat.smooth=yhat.smooth
  #output it
  res
}  #End of reg2() function


#Function 4-
twolines=function(f,graph=1,link="gaussian",data=NULL,pngfile="")  {
  attach(data)

  #(1) Extract variable names
  #1.1 Get the formulas
  y.f=all.vars(f)[1]                  #DV
  x.f=all.vars(f)[2]                  #Variable on which the u-shape shall be tested

  #Number of variables
  var.count=length(all.vars(f))  #How many predictors in addition to the key predictor?
  #Entire model, except the first predictor
  if (var.count>2) nox.f=drop.terms(terms(f),dropx=1,keep.response = T)

  #1.1.5 Drop missing value for any the variables being used
  #all variables in the regression
  vars=all.vars(f)
  #Vector with columns associated with those variable names inthe uploaded dataset
  cols=c()
  for (var in vars) cols=c(cols, which(names(data)==var))
  #Set of complete observations
  full.rows=complete.cases(data[,cols])

  #Drop missing rows
  data=data[full.rows,]
  detach(data)          #Detach the full dataset
  attach(data)          #Attach the one without missing values in key variables


  #1.2 Grab the two key variables to be used, xu and yu
  xu=eval2(x.f)  #xu is the key predictor predicted to be u-shaped
  yu=eval2(y.f)  #yu is the dv

  #1.3 Replace formula for key predictor so that it accommodates  possibly discrete values
  #1.3.1 Count number of unique x values
  unique.x=length(unique(xu))   #How many unique values x has
  #1.3.2 New function segment for x
  sx.f=paste0("s(",x.f,",bs='cr', k=min(10,",unique.x,"))" )

  #2 Run smoother
  #2.1 Define the formula to be run based on whether there are covariates
  if (var.count>2)  gam.f=paste0(format(nox.f),"+",sx.f)      #with covariates
  if (var.count==2) gam.f=paste0("yu~",sx.f)                  #without
  #2.2 Now run it
  gams=gam(as.formula(gam.f),link=link)  #so this is a general additive model with the main specification entered
  #but we make the first predictor, the one that will be tested for having a u-shaped effect
  #be estimated with a completely flexible functional form.



  #(3) Generate yobs (dots)
  #3.1 If no covariates, yobs is the actually observed data
  if (var.count==2) yobs=yu

  #3.2 If covariates present, yobs is the fitted value with u(x) at mean, need new.data() with variables at means
  if (var.count>2) {

    #3.3 Put observed data into matrix
    data.obs=as.data.frame(matrix(nrow=length(xu),ncol=var.count))          #Empty datafile
    colnames(data.obs)=all.vars(f)                                          #Name variables
    for (i in 1:(var.count)) data.obs[,i]=eval(as.name(all.vars(f)[i]))     #fill in data

    #3.4 Drop observations with missing values on any of the variables
    data.obs=na.omit(data.obs)

    #3.5 Create data where xu is at sample means to get residuals based on rest of models to act as yobs
    #Recall: columns 1 & 2 have y and u(x) in obs.data
    data.xufixed    =data.obs
    data.xufixed[,2]=mean(data.obs[,2])   #Note, the 1st predictor, 2nd columns, is always the one hypothesized to be u-shaped
    #replace it with the mean value of the predictor

    #3.6 Get yobs with covariates
    #First the fitted value
    ##add the modified smoother version of x into the formula
    yhat.xufixed=predict.gam(gams,newdata = data.xufixed)        #get fitted values at means for covariates

    #3.7 Substract fitted value from observed y
    yobs = yu-yhat.xufixed

    #3.8 Create data where u(x) is obs, and all else at sample means
    data.otherfixed              = data.obs     #start with original value
    #3.9 Replace all covariates with their mean for fitting data at sample means
    for (i in 3:var.count)  data.otherfixed[,i]=mean(data.obs[,i])
  } #End if covariates are present to compute yobs



  #4) Get the fitted values at sample means for covariates
  #4.1) Get predicted values into list
  if (var.count>2)   g.fit=predict.gam(gams,newdata = data.otherfixed,se.fit=TRUE)  #predict with covariates at means
  if (var.count==2)  g.fit=predict.gam(gams,se.fit=TRUE)

  #4.2) Take out the fitted itself
  y.hat=g.fit$fit
  #4.3) Now the SE
  y.se =g.fit$se.fit


  #5) Most extreme fitted value
  #5.1) Determine if function is at first decreasing (potential u-shape)  vs. increaseing (potentially inverted U)  (potential u-shape) orinverted u shaped using quadratic regression
  #to know if we are looking for max or min

  xu2=xu^2                                                  #Square x term, xu is the 1st predictor re-cpded
  if (var.count>2)  lmq.f=update(nox.f,~xu+xu2+.)           #Add to function with covariates   (put first, before covariates)
  if (var.count==2) lmq.f=yu~xu+xu2                         #
  lmq=lm(as.formula(format(lmq.f)))               #Estimate the quadratic regression
  bqs=lmq$coefficients                            #Get the point estimates
  bx1= bqs[2]                                     #point estimate for effect of x
  bx2=bqs[3]                                      #point estimate for effect of x^2
  x0=min(xu)                                      #lowest x-value
  s0=bx1+2*bx2*x0                                 #estimated slope at the lowest x-value
  if (s0>0)  shape='inv-ushape'                   #if the quadratic is increasing at the lowest point, the could be inverted u-shape
  if (s0<=0) shape='ushape'                       #if it is decreaseing, then it could be a regular u-shape


  #5.2 Get the middle 80% of data to avoid an extreme cutoff
  x10=quantile(xu,.1)
  x90=quantile(xu,.9)
  middle=(xu>x10 & xu<x90)       #Don't consider extreme values for cutoff
  x.middle=xu[middle]

  #5.3 Restrict y.hat to middle
  y.hat=y.hat[middle]
  y.se=y.se[middle]

  #5.4 Find upper and lower band
  y.ub=y.hat+y.se            #+SE is for flat max
  y.lb=y.hat-y.se            #-SE is for flat min

  #5.5 Find most extreme y-hat
  if (shape=='inv-ushape') y.most=max(y.hat)   #if potentially inverted u-shape, use the highest y-hat as the most extrme
  if (shape=='ushape')     y.most=min(y.hat)   #if potential u-shaped, then the lowest instead

  #5.6 x-value associated with the most extreme value
  x.most=x.middle[match(y.most, y.hat)]

  #5.7 Find flat regions
  if (shape=='inv-ushape') flat=(y.ub>y.most)
  if (shape=='ushape')     flat=(y.lb<y.most)
  xflat=x.middle[flat]

  #6 RUN TWO LINE REGRESSIONS
  #6.1 First an interrupted regression at the midpoint of the flat region
  rmid=reg2(f,xc=median(xflat),graph=0)  #Two line regression at the median point of flat maximum

  #6.2  Get z1 and z2, statistical strength of both lines at the midpoint
  z1=abs(rmid$z1)
  z2=abs(rmid$z2)

  #6.3 Adjust breakpoint based on z1,z2
  xc=quantile(xflat,z2/(z1+z2))

  #6.4 Regression split based on adjusted based on z1,z2
  #Save to png? (option set at the beggining by giving png a name)
  if (pngfile!="") png(pngfile, width=2000,height=1500,res=300)
  #Run the two lines
  res=reg2(as.formula(format(f)),xc=xc,graph=graph)
  #Save to png? (close)
  if (pngfile!="") dev.off()



  #7 Add other results obtained before to the output (some of these are read by the server and included in the app)
  res$yobs       = yobs
  res$y.hat      = y.hat
  res$y.ub       = y.ub
  res$y.lb       = y.lb
  res$y.most     = y.most
  res$x.most     = x.most
  res$f          = format(f)
  res$bx1        = bx1           #linear effect in quadratic regression
  res$bx2        = bx2           #quadratic
  res$minx       = min(xu)       #lowest x value
  res$midflat    = median(xflat)
  res$midz1      = abs(rmid$z1)
  res$midz2      = abs(rmid$z2)
  on.exit(detach(data))
  res
} #End function


```

##### Plot {.active}
```{r}
figureS3 = twolines(pref_imp_parenting ~ age, 
                    data = data_included_documented)

nrow(model.frame(figureS3$glm1)) # Sample Size

# First Regression
summary(figureS3$glm1) # Summary with beta coeffiecent and t-value
nrow(model.frame(figureS3$glm1)) - 3 - 1 # Degrees of freedom: 3 predictors - 1

# Second Regression
summary(figureS3$glm2) # Summary with beta coeffiecent and t-value
nrow(model.frame(figureS3$glm2)) - 3 - 1 # Degrees of freedom: 3 predictors - 1


```

#### Level Parenting Intention {.tabset}
##### Code
```{r}
library(mgcv)         #This library has the additive model with smoothing function
library(stringr)      #To process strings in the function
library(sandwich)     #For robust standard errors
library(lmtest)       #To run linear tests
library(scales)

#OUTLINE
#Function 1 - rp(p)       Reformat p-value for printing on chart
#Function 2 - eval2()     Reads a variable name and assigns the values to another variable (useful for processing formulas and creating temporary variables)
#Function 3 - reg2()      Interrupted regression with entered formula plus cutoff point
#Function 4 - twolines()  Run two-lines test, relying on functions 1 and 2 as well

################################################################################
#Function 1 - rp(p)       Reformat p-value for printing on chart
rp=function(p) if (p<.0001)  return("p<.0001")  else  return(paste0("p=", sub("^(-?)0.", "\\1.", sprintf("%.4f", p))))

#Function 2 - eval2()   evaluate a string as a function
eval2=function(string) eval(parse(text=string))  #Function that evaluates a variable"
#if x.f="x1", then x=eval2(x.f)  will populate x with the values of x1

#Function 3 - two-line regression with glm() so that it has heteroskedastic robust standard errors
reg2=function(f,xc,graph=1,family="gaussian")
{
  #Syntax:
  #f: formula as in y~x1+x2+x3
  #The first predictor is the one the u-shape is tested on
  #xc: where to set the breakpoint, a number
  #link:
  #       Gaussian for OLS
  #       binomial for probit

  #(1) Extract variable names from formula
  #1.1 Get the formulas
  y.f=all.vars(f)[1]                  #DV
  x.f=all.vars(f)[2]                  #Variable on which the u-shape shall be tested
  #Number of variables
  var.count=length(all.vars(f))       #How many total variables, including y and x
  #Entire model, except the first predictor
  if (var.count>2) nox.f=drop.terms(terms(f),dropx=1,keep.response = T)


  #1.2 Grab the two key variables to be used, xu and yu
  xu=eval2(x.f)  #xu is the key predictor predicted to be u-shaped
  yu=eval2(y.f)  #yu is the dv

  #1.3 Replace formula for key predictor so that it accommodates possibly discrete values, gam() breaks down if x has few possible values unless one restricts it, done automatically
  #1.3.1 Count number of unique x values
  unique.x=length(unique(xu))   #How many unique values x has
  #1.3.2 New function segment for x
  sx.f=paste0("s(",x.f,",bs='cr', k=min(10,",unique.x,"))" )

  #1.4 xc is included in the first line
  xlow1  =ifelse(xu<=xc,xu-xc,0)   #xlow=x-xc when x<xc, 0 otherwise
  xhigh1=ifelse(xu>xc,xu-xc,0)     #xhigh=x when x<xmax, 0 otherwise
  high1 =ifelse(xu>xc,1,0)         #high dummy, allows interruption

  #1.5 Now include xc in second line
  xlow2=ifelse(xu<xc,xu-xc,0)
  xhigh2=ifelse(xu>=xc,xu-xc,0)
  high2=ifelse(xu>=xc,1,0)

  #(2) Run interrupted regressions
  #2.1 Generate formulas  replacing the single predictor x, with the 3 new variables
  #If there were covariates, grab them an copy-paste them after the 3 new variables
  if (var.count>2)
  {
    glm1.f=update(nox.f,~ xlow1+xhigh1+high1+.)  #update takes a formula and adds elements to it, by putting the . at the end, it will paste the existing variables after the new 3 variables
    glm2.f=update(nox.f,~ xlow2+xhigh2+high2+.)
  }
  #If there were no covariates, just run the 3 variable model
  if (var.count==2)
  {
    glm1.f=as.formula("yu~ xlow1+xhigh1+high1")
    glm2.f=as.formula("yu~ xlow2+xhigh2+high2")
  }
  #2.2 Run them
  glm1=glm(as.formula(format(glm1.f)),family=family)
  glm2=glm(as.formula(format(glm2.f)),family=family)

  #2.3 Compute robust standard errors
  rob1=coeftest(glm1, vcov=vcovHC(glm1,"HC3"))  #Until 2018 03 20 I was using HC3, but they sometimes generate errors, so i switched it to HC1
  rob2=coeftest(glm2, vcov=vcovHC(glm2,"HC3"))

  #Sometimes HC3 gives NA values (for very sparse or extreme data), check and if that's the case change method
  msg=""
  if (is.na(rob1[2,4]))
  {
    rob1=coeftest(glm1, vcov=vcovHC(glm1,"HC1"))
    msg=paste0(msg,"\nFor line 1 the heteroskedastic standard errors HC3 resulted in an error thus we used HC1 instead.")
  }
  if (is.na(rob2[2,4]))
  {
    rob2=coeftest(glm2, vcov=vcovHC(glm2,"HC1"))
    msg=paste0(msg,"\nFor line 2 the heteroskedastic standard errors HC3 resulted in an error thus we used HC1 instead.")
  }

  #2.4 Slopes
  b1=as.numeric(rob1[2,1])
  b2=as.numeric(rob2[3,1])

  #2.5 Test statistics, z-values
  z1=as.numeric(rob1[2,3])
  z2=as.numeric(rob2[3,3])

  #2.6 p-values
  p1=as.numeric(rob1[2,4])
  p2=as.numeric(rob2[3,4])


  #3) Is the u-shape significant?
  u.sig =ifelse(b1*b2<0 & p1<.05 & p2<.05,1,0)

  #4) Plot results
  if (graph==1) {

    #4.1 General colors and parameters
    pch.dot=1          #Dot for scatterplot (data)
    #col.l1='blue2'     #Color of straight line 1
    col.l1='dodgerblue3'
    col.l2='firebrick'      #Color of straight line 2
    col.fit='gray50'   #Color of fitted smooth line

    col.div="green3"   #Color of vertical line
    lty.l1=1           #Type of line 1
    lty.l2=1           #Type of line 2
    lty.fit=2          #Type of smoothed line

    #4.2) Estimate smoother
    if (var.count>2)
    {
      gam.f=paste0(format(nox.f),"+",sx.f)   #add the modified smoother version of x into the formula
      gams=gam(as.formula(gam.f),link=link)  #now actually run the smoother
    }

    if (var.count==2)
    {
      gams=gam(as.formula(paste0("yu~",sx.f)),link=link)  #now actually run the smoother
    }

    #Get dots of raw data
    #4.3) If no covariates, there are two variables, and y.dots  is the y values
    if (var.count==2) yobs=yu

    #4.4) If covariates present, yobs is the fitted value with u(x) at mean, need new.data() with variables at means
    if (var.count>2) {

      #4.4.1) Put observed data into matrix
      data.obs=as.data.frame(matrix(nrow=length(xu),ncol=var.count))
      colnames(data.obs)=all.vars(f)
      #4.4.2 Populate the dataset with the observed variables
      for (i in 1:(var.count)) data.obs[,i]=eval(as.name(all.vars(f)[i]))


      #4.4.3) Drop observations with missing values on any of the variables
      data.obs=na.omit(data.obs)

      #4.4.4) Create data where u(x) is at sample means to get residuals based on rest of models to act as yobs
      #Recall: columns 1 & 2 have y and u(x) in obs.data
      data.xufixed    =data.obs
      data.xufixed[,2]=mean(data.obs[,2])   #Note, the 1st predictor, 2nd columns, is always the one hypothesized to be u-shaped
      #replace it with the mean value of the predictor
      #5.4.5) Create data where u(x) is obs, and all else at sample means
      data.otherfixed = data.obs     #start with original value
      #Replace all RHS with mean, except the u(x)
      #for (i in 3:var.count) data.otherfixed[,i]=mean(data.obs[,i])  #changed on 2018 11 23 to allow having factors() as predictors, their "midpoint" value is used, sometimes that value is more meaningfully a median point than others
      for (i in 3:var.count) {  #loop over covariates
        xt=sort(data.obs[,i]) #create auxiliary variable that has those values sorted
        n=length(xt)          #see how many observations there are
        xm=xt[round(n/2,0)]   #take the midpoint value (this will work with ordinal and factor data, but with factor it can be arbitrary)
        data.otherfixed[,i]=xm #Replace with that value in the dataset used for fitted value
      }
      #4.4.6) Get yobs with covariates
      #First the fitted value
      yhat.xufixed=predict.gam(gams,newdata = data.xufixed)
      #Substract fitted value from observed y, and shift it with constant so that it has same mean as original y
      yobs =yu-yhat.xufixed
      yobs=yobs+mean(yu)-mean(yobs)  #Adjust to have the same mean
    } #End if for covariates that requires computes y.obs instead of using real y.

    #4.5) First line (x,y) coordinates
    #     offset1=mean(yobs[xu<=xc])-min(xu)*b1-(xc-min(xu))/2*b1
    #     x.l1=c(min(xu),xc)
    #     y.l1=c(min(xu)*b1+offset1,xc*b1+offset1)
    #
    #     #4.6) First line (x,y) coordinates
    # 		offset2=mean(yobs[xu>=xc])-xc*b2-(max(xu)-xc)/2*b2
    # 		x.l2=c(xc,max(xu))
    # 		y.l2=c(xc*b2+offset2,max(xu)*b2+offset2)

    #4.7) Get yhat.smooth
    #Without covariates, just fit the observed data
    if (var.count==2)  yhat.smooth=predict.gam(gams)
    #With covariates, fit at observed means
    if (var.count>2)  yhat.smooth=predict.gam(gams,newdata = data.otherfixed)
    #Substract fitted value from observed y
    offset3 = mean(yobs-yhat.smooth)
    yhat.smooth=yhat.smooth+offset3

    #4.8) Coordinates for top and bottom end of chart
    y1   =max(yobs,yhat.smooth)  #highest point
    y0   =min(yobs,yhat.smooth)  #lowest point
    yr   =y1-y0                  #range
    y0   =y0-.3*yr               #new lowest. 30% lower

    #xs
    x1   =max(xu)
    x0   =min(xu)
    xr   =x1-x0

    #4.9) Plot
    #4.9.1) Figure out coordinates for arrows so that they fit

    par(mar=c(5.4,5.4,.5,2.1))
    plot(xu[xu<xc],yobs[xu<xc], cex =.75, pch = 16, col=alpha(col.l1, 0.1),las=1,
         ylim=c(y0,y1),
         xlim=c(min(xu),max(xu)),
         xlab="",
         ylab="",
         yaxt='n', ann=FALSE)  #Range of y has extra 30% to add labels
    points(xu[xu>xc],yobs[xu>xc], cex = .75, pch = 16, col=alpha(col.l2, 0.10))

    #Axis labels
    mtext(side=1,line=2.75,"Age",font=2)
    mtext(side=2,line=2.75,"\nPrefered Level\nPartner's Intention to Become a Parent",font=2)
    axis(1, at = c(20, 25, 30, 35, 40, 45, 50, 55, 60, 65))
    axis(2, at = c(0, 1, 2, 3, 4, 5, 6))
    

    #4.10) Smoothed line
    #lines(xu[order(xu)],yhat.smooth[order(xu)],col=col.fit,lwd=2,lty=lty.fit)

    #4.10 - New 2018 05 25
    lines(xu[order(xu)],yhat.smooth[order(xu)],col=col.fit,lty=2,lwd=2)

    #4.10.2 Arrow 1
    xm1=(xc+x0)/2
    x0.arrow.1=xm1-.1*xr
    x1.arrow.1=xm1+.1*xr
    y0.arrow.1=y0+.1*yr
    y1.arrow.1=y0+.1*yr+ b1*(x1.arrow.1-x0.arrow.1)

    #Move arrow if it is too short
    if (x0.arrow.1<x0+.1*xr) x0.arrow.1=x0

    #Move arrow if it covers text
    gap.1=(min(y0.arrow.1,y1.arrow.1)-(y0+.1*yr))
    if (gap.1<0) {
      y0.arrow.1=y0.arrow.1-gap.1
      y1.arrow.1=y1.arrow.1-gap.1
    }

    arrows(x0=x0.arrow.1,x1=x1.arrow.1,y0=y0.arrow.1,y1=y1.arrow.1,col=col.l1,lwd=2)

    #4.10.3 Text under arrow 1
    xm1=max(xm1,x0+.20*xr)
    text(xm1,y0+.025*yr,
         paste0("Average Slope 1:\nb = ",round(b1,2),", ",rp(p1)),col=col.l1)

    #4.10.3 Arrow 2
    x0.arrow.2=xc+(x1-xc)/2-.1*xr
    x1.arrow.2=xc+(x1-xc)/2+.1*xr
    y0.arrow.2=y1.arrow.1
    y1.arrow.2=y0.arrow.2 + b2*(x1.arrow.2-x0.arrow.2)

    gap.2=(min(y0.arrow.2,y1.arrow.2)-(y0+.1*yr))
    if (gap.2<0) {
      y0.arrow.2=y0.arrow.2-gap.2
      y1.arrow.2=y1.arrow.2-gap.2
    }


    #Shorten arrow if it is too close to the end
    x1.arrow.2=min(x1.arrow.2,x1)
    if (x0.arrow.2<xc) x0.arrow.2=xc

    xm2=xc+(x1-xc)/2
    xm2=min(xm2,x1-.2*xr)


    arrows(x0=x0.arrow.2,x1=x1.arrow.2,y0=y0.arrow.2,y1=y1.arrow.2,col=col.l2,lwd=2)
    text(xm2,y0+.025*yr,
         paste0("Average Slope 2:\nb = ",round(b2,2),", ",rp(p2)),col=col.l2)



    #4.13 Division line
    lines(c(xc,xc),c(y0+.35*yr,y1),col=col.div,lty=lty.fit)
    text(xc,y0+.3*yr,round(xc,2),col=col.div)
    
  }#End: if  graph==1

  #5 list with results
  res=list(b1=b1,p1=p1,b2=b2,p2=p2,u.sig=u.sig,xc=xc,z1=z1,z2=z2,
           glm1=glm1,glm2=glm2,rob1=rob1,rob2=rob2,msg=msg)  #Output list with all those parameters, betas, z-values, p-values and significance for u

  if (graph==1) res$yhat.smooth=yhat.smooth
  #output it
  res
}  #End of reg2() function


#Function 4-
twolines=function(f,graph=1,link="gaussian",data=NULL,pngfile="")  {
  attach(data)

  #(1) Extract variable names
  #1.1 Get the formulas
  y.f=all.vars(f)[1]                  #DV
  x.f=all.vars(f)[2]                  #Variable on which the u-shape shall be tested

  #Number of variables
  var.count=length(all.vars(f))  #How many predictors in addition to the key predictor?
  #Entire model, except the first predictor
  if (var.count>2) nox.f=drop.terms(terms(f),dropx=1,keep.response = T)

  #1.1.5 Drop missing value for any the variables being used
  #all variables in the regression
  vars=all.vars(f)
  #Vector with columns associated with those variable names inthe uploaded dataset
  cols=c()
  for (var in vars) cols=c(cols, which(names(data)==var))
  #Set of complete observations
  full.rows=complete.cases(data[,cols])

  #Drop missing rows
  data=data[full.rows,]
  detach(data)          #Detach the full dataset
  attach(data)          #Attach the one without missing values in key variables


  #1.2 Grab the two key variables to be used, xu and yu
  xu=eval2(x.f)  #xu is the key predictor predicted to be u-shaped
  yu=eval2(y.f)  #yu is the dv

  #1.3 Replace formula for key predictor so that it accommodates  possibly discrete values
  #1.3.1 Count number of unique x values
  unique.x=length(unique(xu))   #How many unique values x has
  #1.3.2 New function segment for x
  sx.f=paste0("s(",x.f,",bs='cr', k=min(10,",unique.x,"))" )

  #2 Run smoother
  #2.1 Define the formula to be run based on whether there are covariates
  if (var.count>2)  gam.f=paste0(format(nox.f),"+",sx.f)      #with covariates
  if (var.count==2) gam.f=paste0("yu~",sx.f)                  #without
  #2.2 Now run it
  gams=gam(as.formula(gam.f),link=link)  #so this is a general additive model with the main specification entered
  #but we make the first predictor, the one that will be tested for having a u-shaped effect
  #be estimated with a completely flexible functional form.



  #(3) Generate yobs (dots)
  #3.1 If no covariates, yobs is the actually observed data
  if (var.count==2) yobs=yu

  #3.2 If covariates present, yobs is the fitted value with u(x) at mean, need new.data() with variables at means
  if (var.count>2) {

    #3.3 Put observed data into matrix
    data.obs=as.data.frame(matrix(nrow=length(xu),ncol=var.count))          #Empty datafile
    colnames(data.obs)=all.vars(f)                                          #Name variables
    for (i in 1:(var.count)) data.obs[,i]=eval(as.name(all.vars(f)[i]))     #fill in data

    #3.4 Drop observations with missing values on any of the variables
    data.obs=na.omit(data.obs)

    #3.5 Create data where xu is at sample means to get residuals based on rest of models to act as yobs
    #Recall: columns 1 & 2 have y and u(x) in obs.data
    data.xufixed    =data.obs
    data.xufixed[,2]=mean(data.obs[,2])   #Note, the 1st predictor, 2nd columns, is always the one hypothesized to be u-shaped
    #replace it with the mean value of the predictor

    #3.6 Get yobs with covariates
    #First the fitted value
    ##add the modified smoother version of x into the formula
    yhat.xufixed=predict.gam(gams,newdata = data.xufixed)        #get fitted values at means for covariates

    #3.7 Substract fitted value from observed y
    yobs = yu-yhat.xufixed

    #3.8 Create data where u(x) is obs, and all else at sample means
    data.otherfixed              = data.obs     #start with original value
    #3.9 Replace all covariates with their mean for fitting data at sample means
    for (i in 3:var.count)  data.otherfixed[,i]=mean(data.obs[,i])
  } #End if covariates are present to compute yobs



  #4) Get the fitted values at sample means for covariates
  #4.1) Get predicted values into list
  if (var.count>2)   g.fit=predict.gam(gams,newdata = data.otherfixed,se.fit=TRUE)  #predict with covariates at means
  if (var.count==2)  g.fit=predict.gam(gams,se.fit=TRUE)

  #4.2) Take out the fitted itself
  y.hat=g.fit$fit
  #4.3) Now the SE
  y.se =g.fit$se.fit


  #5) Most extreme fitted value
  #5.1) Determine if function is at first decreasing (potential u-shape)  vs. increaseing (potentially inverted U)  (potential u-shape) orinverted u shaped using quadratic regression
  #to know if we are looking for max or min

  xu2=xu^2                                                  #Square x term, xu is the 1st predictor re-cpded
  if (var.count>2)  lmq.f=update(nox.f,~xu+xu2+.)           #Add to function with covariates   (put first, before covariates)
  if (var.count==2) lmq.f=yu~xu+xu2                         #
  lmq=lm(as.formula(format(lmq.f)))               #Estimate the quadratic regression
  bqs=lmq$coefficients                            #Get the point estimates
  bx1= bqs[2]                                     #point estimate for effect of x
  bx2=bqs[3]                                      #point estimate for effect of x^2
  x0=min(xu)                                      #lowest x-value
  s0=bx1+2*bx2*x0                                 #estimated slope at the lowest x-value
  if (s0>0)  shape='inv-ushape'                   #if the quadratic is increasing at the lowest point, the could be inverted u-shape
  if (s0<=0) shape='ushape'                       #if it is decreaseing, then it could be a regular u-shape


  #5.2 Get the middle 80% of data to avoid an extreme cutoff
  x10=quantile(xu,.1)
  x90=quantile(xu,.9)
  middle=(xu>x10 & xu<x90)       #Don't consider extreme values for cutoff
  x.middle=xu[middle]

  #5.3 Restrict y.hat to middle
  y.hat=y.hat[middle]
  y.se=y.se[middle]

  #5.4 Find upper and lower band
  y.ub=y.hat+y.se            #+SE is for flat max
  y.lb=y.hat-y.se            #-SE is for flat min

  #5.5 Find most extreme y-hat
  if (shape=='inv-ushape') y.most=max(y.hat)   #if potentially inverted u-shape, use the highest y-hat as the most extrme
  if (shape=='ushape')     y.most=min(y.hat)   #if potential u-shaped, then the lowest instead

  #5.6 x-value associated with the most extreme value
  x.most=x.middle[match(y.most, y.hat)]

  #5.7 Find flat regions
  if (shape=='inv-ushape') flat=(y.ub>y.most)
  if (shape=='ushape')     flat=(y.lb<y.most)
  xflat=x.middle[flat]

  #6 RUN TWO LINE REGRESSIONS
  #6.1 First an interrupted regression at the midpoint of the flat region
  rmid=reg2(f,xc=median(xflat),graph=0)  #Two line regression at the median point of flat maximum

  #6.2  Get z1 and z2, statistical strength of both lines at the midpoint
  z1=abs(rmid$z1)
  z2=abs(rmid$z2)

  #6.3 Adjust breakpoint based on z1,z2
  xc=quantile(xflat,z2/(z1+z2))

  #6.4 Regression split based on adjusted based on z1,z2
  #Save to png? (option set at the beggining by giving png a name)
  if (pngfile!="") png(pngfile, width=2000,height=1500,res=300)
  #Run the two lines
  res=reg2(as.formula(format(f)),xc=xc,graph=graph)
  #Save to png? (close)
  if (pngfile!="") dev.off()



  #7 Add other results obtained before to the output (some of these are read by the server and included in the app)
  res$yobs       = yobs
  res$y.hat      = y.hat
  res$y.ub       = y.ub
  res$y.lb       = y.lb
  res$y.most     = y.most
  res$x.most     = x.most
  res$f          = format(f)
  res$bx1        = bx1           #linear effect in quadratic regression
  res$bx2        = bx2           #quadratic
  res$minx       = min(xu)       #lowest x value
  res$midflat    = median(xflat)
  res$midz1      = abs(rmid$z1)
  res$midz2      = abs(rmid$z2)
  on.exit(detach(data))
  res
} #End function


```

##### Plot {.active}
```{r}
figureS4 = twolines(pref_level_parenting ~ age,
                    data = data_included_documented)

# First Regression
summary(figureS4$glm1) # Summary with beta coeffiecent and t-value
nrow(model.frame(figureS4$glm1)) - 3 - 1 # Degrees of freedom: 3 predictors - 1

# Second Regression
summary(figureS4$glm2) # Summary with beta coeffiecent and t-value
nrow(model.frame(figureS4$glm2)) - 3 - 1 # Degrees of freedom: 3 predictors - 1

```

